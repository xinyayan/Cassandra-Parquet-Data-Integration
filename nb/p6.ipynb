{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b3ac56-92f1-4e5d-8cda-22da4dac19ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datacenter: datacenter1\n",
      "=======================\n",
      "Status=Up/Down\n",
      "|/ State=Normal/Leaving/Joining/Moving\n",
      "--  Address        Load        Tokens  Owns (effective)  Host ID                               Rack \n",
      "UN  192.168.192.4  242.4 KiB   16      75.3%             3f2a2574-f83c-466f-9a52-116ca6cdd389  rack1\n",
      "UN  192.168.192.3  70.26 KiB   16      61.2%             4007d7ae-7d10-4672-98c2-ddbb85060db9  rack1\n",
      "UN  192.168.192.2  242.41 KiB  16      63.5%             93d445fb-58a5-41b3-93f4-29298c2f480d  rack1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc61c5e-30ca-43d4-9274-56b76de2764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['p6-db-1', 'p6-db-2', 'p6-db-3'])\n",
    "cass = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d779f5e0-2b67-4914-9be5-5d6d307f98d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fd2e19c95a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"drop keyspace if exists weather\")\n",
    "cass.execute(\"create keyspace weather with replication={'class': 'SimpleStrategy', 'replication_factor': 3};\")\n",
    "cass.execute(\"use weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6a2473-218e-49c0-a903-59e0f17fa720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fd2e19c8d00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"\"\"\n",
    "CREATE TYPE station_record (tmin INT, tmax INT)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890074a4-5671-4bc6-9901-aafc7beaa9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7fd2e18f2da0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"drop table if exists stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bd9c28-f5bc-43fe-9172-07155689c7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE weather.stations (\\n    id text,\\n    date date,\\n    name text static,\\n    record station_record,\\n    PRIMARY KEY (id, date)\\n) WITH CLUSTERING ORDER BY (date ASC)\\n    AND additional_write_policy = '99p'\\n    AND bloom_filter_fp_chance = 0.01\\n    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}\\n    AND cdc = false\\n    AND comment = ''\\n    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}\\n    AND compression = {'chunk_length_in_kb': '16', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}\\n    AND memtable = 'default'\\n    AND crc_check_chance = 1.0\\n    AND default_time_to_live = 0\\n    AND extensions = {}\\n    AND gc_grace_seconds = 864000\\n    AND max_index_interval = 2048\\n    AND memtable_flush_period_in_ms = 0\\n    AND min_index_interval = 128\\n    AND read_repair = 'BLOCKING'\\n    AND speculative_retry = '99p';\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"\"\"\n",
    "CREATE table stations(\n",
    "    id TEXT,\n",
    "    name TEXT static,\n",
    "    date DATE,\n",
    "    record weather.station_record,\n",
    "    PRIMARY KEY (id, date)\n",
    ") WITH CLUSTERING ORDER BY (date ASC)\n",
    "\"\"\")\n",
    "cass.execute(\"describe table weather.stations\").one().create_statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3966ed-3436-464b-9dc3-f268e69062bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3d43a4bd-0427-475c-804a-1d9b0155351a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.0 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.0/spark-cassandra-connector_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.0!spark-cassandra-connector_2.12.jar (143ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.0/spark-cassandra-connector-driver_2.12-3.4.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0!spark-cassandra-connector-driver_2.12.jar (114ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (276ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (61ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (145ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (37ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (104ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (40ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (28ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (28ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (30ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (30ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (46ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (35ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (31ms)\n",
      ":: resolution report :: resolve 5699ms :: artifacts dl 1277ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.0 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   18  |   18  |   0   ||   18  |   18  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3d43a4bd-0427-475c-804a-1d9b0155351a\n",
      "\tconfs: [default]\n",
      "\t18 artifacts copied, 0 already retrieved (18067kB/109ms)\n",
      "23/11/22 22:17:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"p6\")\n",
    "         .config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.4.0')\n",
    "         .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d96413d4-205e-4581-873f-0a4c0d9127ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "df = spark.read.text(\"ghcnd-stations.txt\")\n",
    "df2 = df.withColumn(\"ID\", expr(\"substring(value, 0, 11)\"))\\\n",
    "    .withColumn(\"STATE\", expr(\"substring(value, 39, 2)\"))\\\n",
    "    .withColumn(\"NAME\", expr(\"substring(value, 41, 30)\"))\\\n",
    "    .filter(col(\"STATE\")=='WI')\n",
    "collected_df = df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a884d1e9-6634-447e-9e01-220cc9c5ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_insert = cass.prepare(\"\"\"\n",
    "INSERT INTO stations (id, name)\n",
    "VALUES (?, ?)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e3f25f-1f94-4650-949e-97d8b3cfa76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in collected_df:\n",
    "    cass.execute(stations_insert, (row.ID, row.NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6231fb6b-7f91-434b-98e3-a0c2abff6477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1313"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"SELECT COUNT(*) FROM weather.stations\").one().count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a06bb54-ec5f-4b5a-9991-56fabb386135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' MADISON DANE CO RGNL AP      '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cass.execute(\"SELECT name FROM stations WHERE id = 'USW00014837'\").one().name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7be41208-baae-445b-8502-e3316037abc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9014250178872933741"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id = cass.execute(\"SELECT token(id) AS id_token FROM stations WHERE id = 'USC00470273'\").one().id_token\n",
    "token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89f60f32-d4cd-4556-8d7e-b99c0a8f30a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8526915793050810996"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "output = subprocess.check_output([\"nodetool\",\"ring\"], encoding='utf-8')\n",
    "vnodes = output.split(\"\\n\")\n",
    "\n",
    "updated_vnodes = []\n",
    "for node in vnodes:\n",
    "    try:\n",
    "        token = int(node.split()[-1])\n",
    "        updated_vnodes.append(node.split())\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "for i, node in enumerate(updated_vnodes):\n",
    "    token = int(node[-1])\n",
    "    if i == 0:\n",
    "        prev_token = int(updated_vnodes[-2][-1])\n",
    "    else:\n",
    "        prev_token = int(updated_vnodes[i - 1][-1])\n",
    "\n",
    "    if (prev_token < token and prev_token < token_id <= token):\n",
    "        vnode = token\n",
    "    elif (prev_token > token and (token_id > prev_token or token_id <= token)):\n",
    "        vnode = token\n",
    "vnode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "035e776a-3776-4c86-a187-737a69d6cb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr, first\n",
    "\n",
    "temp_df = (spark.read\n",
    " .format(\"parquet\")\n",
    " .load(\"records.parquet\")\n",
    ")\n",
    "temp_df = temp_df.groupBy(\"station\", \"date\").pivot(\"element\", [\"TMAX\", \"TMIN\"]).agg(first(\"value\"))\n",
    "temp_df = temp_df.withColumn(\"TMAX\", expr(\"CAST(TMAX AS INT)\")) \\\n",
    "       .withColumn(\"TMIN\", expr(\"CAST(TMIN AS INT)\"))\n",
    "temp_df = temp_df.withColumn(\"date\", expr(\"to_date(`date`, 'yyyyMMdd')\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f674e56b-c3ee-41ad-b715-007e25e751a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused {created_time:\"2023-11-22T22:18:22.461032266+00:00\", grpc_status:14}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m data_df \u001b[38;5;241m=\u001b[39m temp_df\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data_df:\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mstub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecordTemps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecordTempsRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTMAX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTMIN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py:1161\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1148\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1154\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1155\u001b[0m     (\n\u001b[1;32m   1156\u001b[0m         state,\n\u001b[1;32m   1157\u001b[0m         call,\n\u001b[1;32m   1158\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1159\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[0;32m-> 1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grpc/_channel.py:1004\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state\u001b[38;5;241m.\u001b[39mresponse\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1004\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:5440: Failed to connect to remote host: Connection refused {created_time:\"2023-11-22T22:18:22.461032266+00:00\", grpc_status:14}\"\n>"
     ]
    }
   ],
   "source": [
    "import grpc\n",
    "import station_pb2, station_pb2_grpc\n",
    "\n",
    "PORT = 5440\n",
    "channel = grpc.insecure_channel(f'localhost:{PORT}')\n",
    "stub = station_pb2_grpc.StationStub(channel)\n",
    "data_df = temp_df.collect()\n",
    "for row in data_df:\n",
    "    stub.RecordTemps(station_pb2.RecordTempsRequest(station=row.station, date=str(row.date), tmax=row.TMAX, tmin=row.TMIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276c8e1-5348-4247-9486-ecc84949ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stub.StationMax(station_pb2.StationMaxRequest(station='USW00014837')).tmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda4ff0-5a02-4a4b-b744-933a6a068f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".option(\"spark.cassandra.connection.host\", \"p6-db-1,p6-db-2,p6-db-3\")\\\n",
    ".option(\"keyspace\", \"weather\")\\\n",
    ".option(\"table\", \"stations\")\\\n",
    ".load()\n",
    "spark_df.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d3c85-5073-429c-97c7-bea9cab98dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78044e99-8864-45d0-b673-75b87a5cf33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_diff_df = spark.sql(\"\"\"\n",
    "    SELECT id, AVG(record.tmax - record.tmin) as avg_diff\n",
    "    FROM stations\n",
    "    WHERE record is not null\n",
    "    GROUP BY id\n",
    "\"\"\")\n",
    "\n",
    "average_diff_rows = average_diff_df.collect()\n",
    "\n",
    "average_diff_dict = {row['id']: row['avg_diff'] for row in average_diff_rows}\n",
    "\n",
    "average_diff_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86f8d0-8f9a-4ac2-806a-a55e92dd907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nodetool status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e04c71-8d1c-41d7-8207-da29df800230",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = stub.StationMax(station_pb2.StationMaxRequest(station='USW00014837'))\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd152947-f000-4ced-8968-73c114751f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = stub.RecordTemps(station_pb2.RecordTempsRequest(station='USW00014837', date='2022-01-20', tmax=100, tmin=-100))\n",
    "error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
